# Stanford Machine Learning Course by Andrew Ng

This repository holds my completed Octave/Matlab code for the exercises in the course. 


## Week 2 - Linear Regression with Multiple Variables (Exercise 1)
Files for building functions that:
- Explore data using scatter plots.
- Implement Gradient Descent using an appropriate Cost Function.
- Implement Gradient Descent using the closed-from normal equation.
- Normalise features to enhance Gradient Descent's convergence. 
- Use multivariate linear regression to make out-of-sample predictions.

## Week 3 - Logistic Regression (Exercise 2) 
Files for building functions that:
- Implement Gradient Descent on an appropriate cost function. 
- Explore and discover appropriate weights with-which to regularise the Logistic Regression. 
- Implement a regularized version of Logistic Regression by using a modified cost function.  
- Deploy and use Logistic Regression to make out-of-sample predictions.

## Exercise 3 - Neural Networks: Representation (Week 4)
Files for building functions that:
- Build multi-class classifier composed of multiple linear regressions
- transform the data using a sigmoid link function to produce logistic regression
- Deploy the "One vs. All" method to classify handwritten digits and make out-of-sample classifications.
- Build a Feed-forward neural network with three layers.
- Train the neural network weights and biases on hand-written digits, storing the weights in a matrix
- Use the network to classify out-of-sample digits.

## Exercise 4 - Neural Networks: learning (Week 5)
- Implement a Feed-forward Neural Network using backprpagation from scratch to classify hand-written digits.
- Use Finite-Differences method for Gradient Checking.
- Enhance Neural Network performance and avoid over-fitting by implementing regularization.

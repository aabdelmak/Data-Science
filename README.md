# Stanford Machine Learning Course by Andrew Ng

This repository holds my completed Octave/Matlab code for the exercises in the course. 


## Week 2 - Linear Regression with Multiple Variables (Exercise 1)
Files for building functions that:
- Explore data using scatter plots.
- Implement Gradient Descent using an appropriate Cost Function.
- Implement Gradient Descent using the closed-from normal equation.
- Normalise features to enhance Gradient Descent's convergence. 
- Use multivariate linear regression to make out-of-sample predictions.

## Week 3 - Logistic Regression (Exercise 2) 
Files for building functions that:
- Implement Gradient Descent on an appropriate cost function. 
- Explore and discover appropriate weights with-which to regularise the Logistic Regression. 
- Implement a regularized version of Logistic Regression by using a modified cost function.  
- Deploy and use Logistic Regression to make out-of-sample predictions.

## Week 4 - Neural Networks: Representation (Exercise 3)
Files for building functions that:
- Build multi-class classifier composed of multiple linear regressions
- transform the data using a sigmoid link function to produce logistic regression
- Deploy the "One vs. All" method to classify handwritten digits and make out-of-sample classifications.
- Build a Feed-forward neural network with three layers.
- Train the neural network weights and biases on hand-written digits, storing the weights in a matrix
- Use the network to classify out-of-sample digits.

## Week 5 - Neural Networks: learning (Exercise 4)
- Implement a Feed-forward Neural Network using backprpagation from scratch to classify hand-written digits.
- Use Finite-Differences method for Gradient Checking.
- Enhance Neural Network performance and avoid over-fitting by implementing regularization.

## Week 6 - 
- Implement regularized linear regression and use learning and validation curves to Debug learning algorithm.
- Compute and normalize polynomial features to experiment with polynomial regressions of various degrees, d, and determine the most appropriate value of d based on the  bias-variance tradeoff.
- Experiment with multiple regularization parameters and use the learning and validation curves to determine optimal bias-variance trade-off.
- 

